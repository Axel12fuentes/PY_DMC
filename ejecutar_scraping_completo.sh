#!/bin/bash

# Script de ejecuciÃ³n para el scraping masivo de 12 plataformas educativas
# Autor: Sistema de Scraping LLM-Enhanced
# Fecha: 2026-01-18

echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘   ğŸš€ SCRAPER MASIVO DE PLATAFORMAS EDUCATIVAS               â•‘"
echo "â•‘   12 Sitios | LLM-Powered (GPT-4o) | CSV Consolidado       â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

# Verificar entorno virtual
if [ ! -d "dmc_env" ]; then
    echo "âŒ Error: Entorno virtual no encontrado"
    echo "   Ejecuta primero: python3 -m venv dmc_env"
    exit 1
fi

# Activar entorno
source dmc_env/bin/activate

# Verificar dependencias
echo "ğŸ“¦ Verificando dependencias..."
pip install -q pandas playwright openai pdfplumber python-dotenv

# Verificar API Key
if [ -z "$OPENAI_API_KEY" ] && [ ! -f ".env" ]; then
    echo "âš ï¸  ADVERTENCIA: OPENAI_API_KEY no encontrada"
    echo "   AsegÃºrate de tener un archivo .env con tu API key"
    read -p "   Â¿Continuar de todas formas? (y/n): " -n 1 -r
    echo
    if [[ ! $REPLY =~ ^[Yy]$ ]]; then
        exit 1
    fi
fi

echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "  INICIANDO SCRAPING DE 12 PLATAFORMAS"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
echo "ğŸ“‹ Plataformas a scrapear:"
echo "   [0-4]  Ya implementadas (5): Datapath, DataScience, DMC, SmartData, NewHorizons"
echo "   [5-12] Nuevas (7): BSG, WE, PUCP x3, UPC, ED Team, Platzi"
echo ""
echo "â±ï¸  Tiempo estimado: 4-8 horas"
echo "ğŸ’° Costo estimado: ~$10-30 USD (GPT-4o)"
echo "ğŸ“Š Cursos esperados: 500-1500+ cursos"
echo ""

# Preguntar confirmaciÃ³n
read -p "ğŸš¦ Â¿Deseas continuar con el scraping COMPLETO? (y/n): " -n 1 -r
echo
echo ""

if [[ $REPLY =~ ^[Yy]$ ]]; then
    echo "ğŸ¯ Ejecutando scraping completo..."
    echo ""
    
    # Ejecutar con PYTHONPATH
    PYTHONPATH=. python3 run_all_scrapers.py --all
    
    echo ""
    echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
    echo "â•‘  âœ… PROCESO COMPLETADO                                        â•‘"
    echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
    echo "ğŸ“ Archivos generados:"
    echo "   â€¢ output/*_database.csv (CSVs individuales por plataforma)"
    echo "   â€¢ output/MASTER_courses_database_*.csv (CSV consolidado)"
    echo ""
    echo "ğŸ‘€ Revisa la carpeta 'output/' para ver los resultados"
    
else
    echo ""
    echo "âŒ Scraping cancelado por usuario"
    echo ""
    echo "ğŸ’¡ Puedes ejecutar sitios individuales con:"
    echo "   PYTHONPATH=. python3 run_all_scrapers.py --site 0"
    echo ""
    echo "ğŸ“‹ O ver la lista completa:"
    echo "   PYTHONPATH=. python3 run_all_scrapers.py"
fi

deactivate
